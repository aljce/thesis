\documentclass[./Thesis.tex]{subfiles}
\begin{document}

\chapter{Introduction}
\label{chap:Introduction}
Imagine a world where teachers assigning proof filled math homework need not
grade that homework or a world where journals need not peer review their
submissions. This is a world \textit{proof assistants} attempt to bring about. A world
where computers can ensure the correctness of mathematics. This thesis is in
part an exploration of a specific proof assistant, \Agda{} \cite{agda}. As we
will come to see \Agda{} contains an impressive capability for mathematical
abstraction and can come to understand arbitrarily complex proofs. Unfortunately
it does not come without drawbacks. We find it useful to consider \Agda{} as an
infinitely intelligent toddler. An actor that knows nothing until it is taught
and requires convincing at every step. This is because \Agda{} does not have any
specific mathematics built into itself and that it does not perform any
\textit{proof search}. It simply accepts or rejects a given proof. As proof
assistants are useless without something to prove we turn to the other half of
this thesis, investigating prime numbers. \\

Prime numbers are of great interest to mathematicians and computer scientists
the world over. They form the backbone of a variety of fields of mathematics and
underpin the correctness of many real-world cryptographic protocols. In this
thesis we will explore formalizing primality tests, or algorithms that determine
if a given input $n$ is prime. These tests and their speed are critical to the
implementation of the cryptographic protocols mentioned above. Specifically the RSA
cryptosystem, which is a vital component of the world wide web, requires these
tests \cite{rsa}.
\section{Primality Tests}
There are many different primality tests and many authors choose to characterize
these tests along three dimensions. These dimensions are as follows
\textit{computational complexity} (how fast the algorithm runs),
\textit{determinism} (if the algorithm makes use of probabilistic primitive
operations), and \textit{conditionality} (if the algorithm requires unproven
conjectures in its proof of correctness or speed). Let us first consider the
brute force primality test. The algorithm determines the primality of a given
input $n$ by testing every number $m$ less than $n$ to see if $m$ divides $n$.
If no numbers divides $n$ then $n$ is prime. This test is both deterministic and
unconditional but suffers one major flaw, its computational complexity is
$\BigO{n}$. At first glance this does not seem bad but for any real world prime
this becomes untenable. For instance say we tried to test the primality of any
prime used in RSA. These primes hover around $2^{4096}$ in size and even at a
rate of 10 billion divisibility checks a second it would take $100$ times the
length of the universe to finish. Thus a computational complexity that is
polynomial in the size of the input $\BigO{\Lg{n}^c}$ is desired. \\

In 1975, a deterministic polynomial time primality test was designed by Miller.
Unfortunately this test required the
\textit{Extended Riemann Hypothesis} a famous unproven conjecture \cite{miller}.
Thus the Miller primality test is considered conditional. A similar
unconditional primality test was later developed by Rabin but the algorithm was
probabilistic \cite{rabin}. Interestingly the probability of failure for this
algorithm is quite low so many still use this test for real world primality
testing. In 1983, the authors Adleman, Pomerance, and Rumely created the first
unconditional deterministic sub-exponential primality test with a run-time of
$\BigO{\Lg{n}^{c \, \text{lg} \, \text{lg} \, \text{lg} \, n}}$ \cite{apr}. \\

The string of accomplished authors shows both the difficulty and importance of this
problem. Thus it was big news when Manindra Agrawal, Neeraj Kayal,and Nitin
Saxena announced that they had created a unconditional deterministic
primality test that runs in polynomial time $\BigO{\Lg{n}^{15/2}}$ \cite{aks}.
This accomplishment is especially impressive considering that the last two
authors where undergraduate researches at the time of publication (2002). The
last names of these authors are abbreviated to yield the name of this algorithm,
AKS. The correctness proof of this algorithm is comparably simpler to all the
previous algorithms listed above, requiring only undergraduate levels of number
theory. This property makes AKS a great candidate for a ``replication'' in
\Agda{}. Replications are common in various scientific fields and amount to
recreating a previously reported experiment. Usually the goal of a replication
is to verify the correctness of the data and conclusions of the previous
experiment. In a similar vein, if we implement AKS in \Agda{} and \Agda{}
accepts our implementation then we will have verified the correctness of the AKS
paper. \\

Unfortunately this is no small task as \Agda{} does not know any number theory.
We will then have to formulate every piece of number theory and more generally
mathematics used in the AKS paper. Specifically we will have to formalize these
concepts in the \Agda{} proof assistant. In this thesis we will illustrate only
a selection of these concepts as this is a undergraduate time limited thesis.
This being said we estimate to have completed formalizing 75\% of the required
concepts and this work can be found in the following git repository
\url{https://github.com/mckeankylej/thesis}. The required and completed concepts
include but are not limited to a gcd function that works over an arbitrary
euclidean domain, a polynomial datatype that forms a euclidean domain,
a modular ring datatype, a proven correct exponentiation by squaring algorithm,
and a brute force primality test. In this thesis we will specifically cover the
last two concepts in great detail. \\

In order to explain the intricacies of these constructions we must first
introduce the reader to the \Agda{} proof assistant and we accomplish this in
Chapter \ref{chap:agda-briefly}. Then we move on to describe an exponentiation
by squaring algorithm written \Agda{}. This algorithm is required multiple times
in AKS. It constitutes the very last step of the algorithm in which the input
must be tested to ensure it is not a perfect power \cite{smid}. After this in
Chapter \ref{chap:termination} we take a brief aside in order to discuss how
\Agda{} ensures the correctness of recursive programs. Finally in Chapter
\ref{chap:primality} we create a brute force primality test in \Agda{}. The AKS
algorithm requires a list of ``small'' primes so this algorithm will be used to
generate that list.
\section{Type Theory}
In the rest of this Chapter we provide an informal motivation of Type Theory for a reader
familiar with the standard foundations of mathematics, Set Theory. Our proof
assistant \Agda{} is a proof assistant based on Type Theory. For readers that
desire a more formal specification of Type Theory we recommend the following
sources: a topological perspective \cite{hott-book}, a categorical perspective
\cite{nlab}, and a programming language theory perspective \cite{harper}.
\subsection{History}
\label{sec:history}
Type Theory was created by Bertrand Russell as an alternate foundation for
mathematics in order to, quash the plethora of
paradoxes plaguing Set Theory at the turn of the century. The paradoxes
where eventually resolved and type theory faded into obscurity. It resurfaced in
the 1940's when Alonzo Church applied it to his \textit{lambda calculus}. This
application began a reassurance in which multiple authors steadily improved the
expressivity of Type Theory. This culminated with Per Martin-L\"{o}f's
dependent, intuitionistic type theory often called
\textit{Martin-L\"{o}f Type Theory} \cite{martin-lof}.
This theory is considered by many as the foundations of the field. The
programming language \Agda{} that we will come to know in this thesis is a
variant of his Type Theory. This being said the field is vast and there are many
Type Theories created with wildly different axioms and properties in mind.
\subsection{The Typing Relation}
\label{sec:typing-relation}
The primitive concept underlying Type Theory is the \textit{typing relation},
namely a \textit{term} $a$ has \textit{type} $A$ written $a : A$. In the
language of Set Theory this would represent ``a is an element of the set A''
denoted $a \in A$. Despite these similarities types differ from sets in many
ways. For instance the perfectly valid set
$\{ \, \text{true}, \, 0, \, \emptyset \, \}$
is inexpressible in Type Theory as types are ``homogeneous''. More importantly
there is not an equivalent concept of a non-membership ($\notin$) in most Type
Theories. Instead we say $2$ does not have type boolean or we say $2$ is not
well typed as a boolean. Thus while its sensible to ask the set theoretic question
``Is $a$ a member of $A$ for some pre-existing objects $a$ and $A$?''. Type
theoretically we are not permitted to consider objects without their type and
every object has a type from creation. For instance we can define the booleans
($ùîπ$) as two constants $\text{true} : ùîπ$ and $\text{false} : ùîπ$.
\subsection{Propositions as Types}
In reality Set Theory is actually a dual theory. There is a base theory called
first order logic in which a collection of axioms are developed that describe
the structure of sets. So Set Theory is really the study of these two
interleaved theories. Type Theory is its own logic which collapses the two
levels into one. The two basic elements of Set Theory, logical propositions and
sets thus also collapse together. They collapse together into types and
propositions become manipulatable objects. For instance
a function $f$ is injective if the following proposition is true.
\begin{align}
  \text{for any } x, y \text{ if } f(x) = f(y) \text{ then } x = y
\end{align}
We can represent this proposition type theoretically with the following
expression. 
\begin{equation}
  \label{eqn:injective}
  \text{Injective}(f : A \to B) :\equiv \prod_{x : A} \prod_{y : A} \, f(x) \equiv_B f(y) \to x \equiv_A y
\end{equation}
Now in order to prove some function $f$ is injective we simply construct a proof
that has the type specified above, for instance $p : \text{Injective}(f)$.
Thus proving a proposition is simply finding an element of some type.
Also note that proofs are values and so they can be
manipulated like any other value. This concept is called
\textit{proof relevance} and it is essential to Type Theory and this thesis.
\subsection{Intuitionistic Logic}
In this section we investigate the deep consequences of proof relevance.
Consider the Collatz conjecture, concerning the sequence that follows, start with some
integer $n$ and add $n$ to the sequence. If $n$ is even then divide by two
otherwise multiply $n$ by $3$ and add $1$. The result of this computation is the
next element of the sequence. Repeat the process above. The conjecture is
that for any starting value $n$ the sequence eventually reaches $1$. This
conjecture is a well known unsolved problem in mathematics and as such the truth
value of the proposition is unknown. Now consider the following proposition
often called the law of the excluded middle.
\begin{align}
  \label{eqn:set-LEM}
  \text{for any proposition } p \text{ either } p \text{ or } \neg p
\end{align}
We can transform this proposition into Type Theory just as we have before. Note
that propositions are types so we can easily quantify over every proposition by
quantifying over every type.
\begin{align}
  \label{eqn:type-LEM}
  \text{LEM} :\equiv \prod_{p  : \text{Type}} p \vee \neg p
\end{align}
In Set Theory the law of the excluded middle is assumed as an axiom of first
order logic. Presumably Type Theorists could assume it as well and there would
be some value $\text{lem} : \text{LEM}$ representing the axiom. This axiom behaves weirdly
with proof relevance in mind. Consider using this axiom to construct a proof
that the Collatz conjecture is true or false. We simply instantiate $p$ to our
type theoretic encoding of the Collatz conjecture. Thus if we inspect this proof
term we can learn if the Collatz conjecture is true or false. As solving math
problems is not this easy we must not assume the law of the excluded middle.
This variant of logic is called \textit{intuitionistic logic}. \\

This being said the law of the excluded middle appears often
and is a useful axiom. Thankfully the axiom is often provable when you choose a
specific $p$. Imagine instantiating $p$ with the proposition
$n \equiv_{\mathbb{N}} m$ for some $n$ and $m$.
The resulting type is described below.
\begin{align}
  \label{eqn:nat-dec}
  \text{Equals?} :\equiv (n \equiv_{\mathbb{N}} m) \vee \neg (n \equiv_{\mathbb{N}} m)
\end{align}
This is the type of our first decision procedure. A value of this type would
prove that the two numbers are equal or prove that the two numbers are unequal.
Finding a value of this type is as simple as describing a decision procedure.
Simply count $n$ and $m$ down by $1$. If both numbers reach $0$ at the same time
return that they are equal otherwise return that they are unequal. \\

Keep in mind that assuming the law of the excluded middle also invalidates the
entire purpose of this thesis. We can define a proposition that represents
primality (and in fact we do in Chapter \ref{chap:primality}) apply the law of
the excluded middle to this type and now we have a term that ``computes''
primality.
\end{document}
